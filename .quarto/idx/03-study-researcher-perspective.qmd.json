{"title":"Data donation study - researcher perspective","markdown":{"yaml":{"format":{"revealjs":{"footer":"Data Donation Studies - COMPTEXT - Frieder Rodewald, Valerie Hase","center-title-slide":false,"theme":["theme/q-theme.scss"],"code-fold":"show","code-copy":true,"code-block-height":"670px","self-contained":true,"number-sections":false,"smaller":true,"highlight-style":"atom-one","preview-links":"auto","progress":true}},"execute":{"echo":true},"bibliography":"references/references.bib","csl":"references/apa.csl"},"headingText":"Data donation study - researcher perspective","containsRefs":false,"markdown":"\n\n<h1>Follow the User?!</h1>\n\n<h3>Data Donation Studies for Collecting Digital Trace Data</h3>\n\n<hr>\n\nSession 3Ô∏è‚É£: Data Donation Studies (Researcher Perspective)\n\nFrieder Rodewald (University of Mannheim) & Valerie Hase (LMU Munich)\n\n<hr>\n\nüëâ Part of the SPP DFG Project [Integrating Data Donations in Survey Infrastructure](https://data-donation-science.de)\n\n##\n\n::: {style=\"font-size: 200%;text-align:center;\"}\n*What are methodological decisions researchers have to take in data donation studies?* ü§î\n:::\n\n\n![Figure. Data donation study - researcher perspective](images/ablauf-4.jpg){fig-alt=\"process of data donation study\" fig-align=\"left\" width=\"300\"}\n\n## Agenda\n\n1. **Research design & tool set-up**\n\n2. **Data cleaning & augmentation**, including\n\n   üì¢ **Task 3**: Classify search terms\n\n3. **Modelling digital traces**\n\n![Image by Hope House Press via Unsplash](images/agenda.jpg){fig-align=\"left\" width=\"10%\"}\n\n## 1) Research design & tool set-up (Frieder)\n\n![Source: Image by Markus Winkler via Unsplash](images/methods.jpg){fig-alt=\"image of lupe\" fig-align=\"left\" width=\"300\"}\n\n## Step I: Research design & tool set-up\n\n![Figure. Data donation study - researcher perspective](images/ablauf-4a.jpg){fig-alt=\"process of data donation study\" fig-align=\"left\" width=\"300\"}\n\n## Step I: Research design & tool set-up\n\nKey decisions:\n\n-   Which theoretical questions do I want to answer?\n-   How do I operationalize key variables via my data donation tool?\n-   How do I integrate the tool in surveys & recruit participants?\n\n## Step I: Research design & tool set-up\n\nKey decisions:\n\n-   **Which theoretical questions do I want to answer?**\n-   How do I operationalize key variables via my data donation tool?\n-   How do I integrate the tool in surveys & recruit participants?\n\n## Step I.I Which questions do I want to answer?\n\nThis may sound silly but:\n\n-   Novel method, few empirical applications\n-   To date: methodological playground\n-   *What good is a method that is not used to advance theories/empirical knowledge?*\n\n::: {.notes}\n\n- If you have studied Twitter in the 2010's you know that this is pretty much what went wrong there, i.e., most often researchers had a nice data set and then looked for a suitable question to study.\n-> But the scientific way is better the other way around!\n-> Here it also makes a lot of sense to pre-register your study. You will also see in a moment why/how this helps you during programming.\n\n:::\n\n## Step I: Research design & tool set-up\n\nKey decisions:\n\n-   Which theoretical questions do I want to answer?\n-   **How do I operationalize key variables via my data donation tool?**\n-   How do I integrate the tool in surveys & recruit participants?\n\n## Step I.II: How do I operationalize key variables?\n\nChoose a tool, e.g., ...\n\n-   Port [@boeschoten_etal2023] (Netherlands, different platforms)\n-   Data Donation Module [@pfiffner_data_2022] (Switzerland, different platforms)\n-   WhatsR [@kohne_etal2024] (Germany, WhatsApp)\n\n::: {.notes}\n\n- Or you can program your own tool for your own application.\n- A good starting point is to look for data donation studies that were on the same platform; and use their scripts.\n\n:::\n\n## Step I.II: How do I operationalize key variables?\n\n-   Participants \"upload\" data\n-   Local extraction, anonymization, & aggregation\n-   Users can delete data\n-   Informed consent, only then: send to researcher server\n\n::: {.notes}\n\n- From a researcher perspective this makes it also appealing for ethics and data protection\n-> More cumbersome to explain but once they understand it, they like it.\n\n:::\n\n\n## Step I.II: How do I operationalize key variables?\n\n-   Participants \"upload\" data\n-   Local **extraction**, anonymization, & aggregation\n-   Users can delete data\n-   Informed consent, only then: send to researcher server\n\n## Step I.II: How do I operationalize key variables?\n\n<h3>Extractionüîé:</h3>\n\n![Figure. Filtering data - File extraction](images/extraction.jpg){fig-alt=\"Files in data donation packages\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\n<h3>Extractionüîé:</h3>\n\n![Figure. Filtering data - Python code](images/filtering.jpg){fig-alt=\"Python code for extracting files\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\n<h3>Extractionüîé:</h3>\n\n![Figure. Filtering data - Python code](images/filtering-a.jpg){fig-alt=\"Python code for extracting files\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\n-   Participants \"upload\" data\n-   Local extraction, **anonymization**, & aggregation\n-   Users can delete data\n-   Informed consent, only then: send to researcher server\n\n## Step I.II: How do I operationalize key variables?\n\n<h3>Anonymization üôà:</h3>\n\n![Figure. Anonymization - Example of Whitelists](images/anonymization.jpg){fig-alt=\"Example whitelists for news outlets\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\n<h3>Anonymization üôà:</h3>\n\n![Figure. Example of anonymized data](images/anonymization-a.jpg){fig-alt=\"Example of anonymized data\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\n-   Participants \"upload\" data\n-   Local extraction, anonymization, & **aggregation**\n-   Users can delete data\n-   Informed consent, only then: send to researcher server\n\n::: {.notes}\n\n- This often goes hand-in-hand with anonymization, but requires you to know your research question: If you aggregate too much here, you might not have all the information you need for your analyses.\n\n:::\n\n## Step I.II: How do I operationalize key variables?\n\n<h3>Aggregation üßÆ:</h3>\n\n![Figure. Aggregation - Python code](images/filtering-a.jpg){fig-alt=\"Python code for aggregation\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\n-   Participants \"upload\" data\n-   Local extraction, anonymization, & aggregation\n-   Users can **delete data**\n-   Informed consent, only then: send to researcher server\n\n## Step I.II: How do I operationalize key variables?\n\n<h3>Data deletion by users ‚ùå:</h3>\n\n![Figure. Data deletion](images/deletion.jpg){fig-alt=\"Example of how users can delete their data\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\nThis is how much \"fun\" testing DDTs is:\n\n![Figure. Github issues - Testing the tool](images/testing.jpg){fig-alt=\"Github screenshot of testing\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\nKey issues üö® [@hase_etal2024]\n\n-   Missing documentation by platforms (e.g., file structure)\n-   Sudden changes in DDPs\n-   Differences across languages & devices\n-   Insufficient in-tool classification (e.g., LLM integration)\n\n::: {.notes}\n\n- It might be nice to directly classify information during extraction (also in terms of data protection), but since all the data is extracted on the participants devices, this makes it really hard to implement because they would have to do the API calls from their devices, handle the computing power,...\n\n:::\n\n##\n::: {style=\"font-size: 200%;text-align:center;\"}\n**Let's have a look at the technical set-up üíª:**\n\n\\\n\\\n\n[https://github.com/eyra/mono](https://github.com/eyra/mono)\n[https://github.com/eyra/feldspar](https://github.com/eyra/feldspar)\n\n:::\n\n::: {.notes}\n\n- The example flow is here: \"[https://next.eyra.co/assignment/335/content](https://next.eyra.co/assignment/335/content)\"\n- This is just one example for a data donation tool, but arguably the most elaborate platform out there (that you can self-host)\n\n:::\n\n<!--\n---\n\n![Video. Next settings](images/next_settings.mov){fig-alt=\"Next settings video\" fig-align=\"left\" width=\"1400\"}\n\n---\n\n![Video. Next workflow](images/next_workflow.mov){fig-alt=\"Next workflow video\" fig-align=\"left\" width=\"1400\"}\n\n-->\n\n---\n\n![Figure. Next setup 1](images/next_1.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 2](images/next_2.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 3](images/next_3.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 4](images/next_4.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 5](images/next_5.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 6](images/next_6.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 7](images/next_7.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 8](images/next_8.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 9](images/next_9.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 10](images/next_10.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n::: {.notes}\n\n- ...and how does this actually work: Pyodide; we just install python in a participants browser. That how we can code python and work with a language that is more suited to data wrangling than JavaScript\n\n:::\n\n## Strategy to make the extraction work\n\n1. Take a look at the DDP; download it, best for multiple time periods and for different languages\n2. Understand the structure of the JSON or CSV.\n3. Get an example file running.\n4. Write the code for the extraction script.\n5. Test your script, first locally and then in the wild.\n6. Adapt your script.\n\n## Example: Extract list of subscriptions\n\n![Figure. subscriptions.csv](images/subscriptions_csv.jpeg.png){fig-alt=\"Screenshot of subscriptions.csv\" fig-align=\"left\" width=\"200\"}\n\n---\n\n<div style=\"font-size: 1.5em;\">\n\n```{.python}\n...\n    \"subscriptions\": {\n        \"extraction_function\": ef.extract_subscriptions,\n        \"possible_filenames\": [\"Abos.csv\", \"subscriptions.csv\"],\n        \"title\": {\n            \"en\": \"Which channels are you subscribed to?\",\n            \"de\": \"Welche Kan√§le haben Sie abonniert?\",\n            \"nl\": \"Op welke kanalen ben je geabonneerd?\",\n        },\n    },\n...\n\n```\n\n</div>\n\n---\n\n<div style=\"font-size: 1.5em;\">\n\n```{.python}\ndef extract_youtube_content_from_zip_folder(zip_file_path, possible_filenames):\n    \"\"\"Extract content from YouTube data export zip file using filenames\"\"\"\n\n    try:\n        with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n            # Get the list of file names in the zip file\n            filenames = zip_ref.namelist()\n            # Look for matching files\n            for possible_filename in possible_filenames:\n                for filename in filenames:\n                    if possible_filename in filename:\n                        try:\n                            # Process based on file extension\n                            if filename.endswith(\".json\"):\n                                with zip_ref.open(filename) as json_file:\n                                    json_content = json.loads(json_file.read())\n                                    return json_content\n                            elif file_name.endswith(\".csv\"):\n                                with zip_ref.open(file_name) as csv_file:\n                                    csv_content = pd.read_csv(csv_file)\n                                    return csv_content\n\n                        # Try the next matching file if there's an error\n                        except Exception as e:\n                            print(f\"Error reading file {file_name}: {e}\")\n                            continue\n            # If we've checked all files and found no match\n            print(f\"No file matching file '{possible_filenames}' found\")\n            return None\n    except Exception as e:\n        print(f\"Error extracting YouTube content: {e}\")\n        return None\n```\n</div>\n\n---\n\n<div style=\"font-size: 1.5em;\">\n\n```{.python}\ndef extract_subscriptions(subscriptions_csv): # csv file is read before\n    \"\"\"Extract YouTube channel subscriptions\"\"\"\n\n    # Define column name\n    if \"Kanaltitel\" in subscriptions_csv.columns:  # language sensitive\n        channel_column = \"Kanaltitel\"\n    else:\n        channel_column = \"Channel Title\"\n\n    # Define description\n    channel_name = \"Subscribed Channel\"\n\n    # Create DataFrame with just the channel names\n    subscriptions_df = pd.DataFrame({channel_name: subscriptions_csv[channel_column]})\n\n    return subscriptions_df\n```\n</div>\n---\n\n![Figure. Processed subscriptions.csv](images/subscriptions_donated.png){fig-alt=\"Screenshot of the processed subscriptions.csv\" fig-align=\"left\" width=\"200\"}\n\n## Step I: Research design & tool set-up\n\nKey decisions:\n\n-   Which theoretical questions do I want to answer?\n-   How do I operationalize key variables via my data donation tool?\n-   **How do I integrate the tool in surveys & recruit participants?**\n\n## Step I.III: How do I integrate the tool in surveys & recruit participants?\n\n-   Often: survey, then forwarding to an external site\n-   Less often: Integration in existing survey infrastructure [@haim_integrating_2023]\n\n::: {.notes}\n\n- The often use setup of \"first survey, then donation\" has the advantage that you know who participated in your study and then understand how your final sample of people who actually donate their data is biased.\n\n:::\n\n## Step I.III: How do I integrate the tool in surveys & recruit participants?\n\n::: incremental\n-   Low response rates [e.g., @hase_haim2024a; @keusch_etal2024]\n\n    - Behavioral intentions as \"willingness to donate\" high (79-52% of survey respondents)\n    - Actual behavior as \"participation in data donation\" low (37-12% of survey respondents)\n    - Well known intention-behavior gap [@kmetty_etal2025]\n\n-   Non-response bias\n\n-   Primary used in non-probability panels (e.g. online access panels)\n\n-   Survey design strategies: For now, ü§ë is the only thing that works.\n\n-   üëâ Again, we will talk about this in session 4Ô∏è‚É£.\n\n:::\n\n::: {.notes}\n\n- As you see this method, like every method, has its shortcomings.\n\n:::\n\n## Step I: Research design & tool set-up\n\n![Figure. Data donation study - researcher perspective](images/ablauf-4a.jpg){fig-alt=\"process of data donation study\" fig-align=\"left\" width=\"300\"}\n\n## Step II: Data cleaning & augmentation (Valerie)\n\n![Figure. Data donation study - researcher perspective](images/ablauf-4b.jpg){fig-alt=\"process of data donation study\" fig-align=\"left\" width=\"300\"}\n\n\n## Step II.I: How do I clean and extend data?\n\nThis is how your data may look like:\n\n![Figure. Donated data - example](images/cleaning_1.jpg){fig-alt=\"Example of donated data\" fig-align=\"left\" width=\"300\"}\n\n## Step II.I: How do I clean and extend data?\n\nThis is how your data may look like:\n\n![Figure. Donated data - example](images/cleaning_2.jpg){fig-alt=\"process of data donation study\" fig-align=\"left\" width=\"300\"}\n\n## Step II.I: How do I clean and extend data?\n\n-   Manual annotation by participants during data donation\n-   APIs/scraping to extend collected data\n-   Text-as-data methods for classification\n\n##\n::: {style=\"font-size: 100%;text-align:center;\"}\n**üì¢ Task 3: Classify search terms**\n\n*Download the data for Task 4 from the workshop website. This contains YouTube searches collected from a German social media sample. Either discuss this (no-code group) or do this in R/Python (code group).....*\n\n(1) How you would clean the data?\n\n(2) How you would identify health-related searches using NLP methods?\n\n:::\n\n![Figure. Donated data - example](images/sample-yt.jpg){fig-alt=\"Example of YouTube searches\" fig-align=\"left\" width=\"100\"}\n\n## Step II.II: How do I check for bias?\n\n-   Errors in representation and measurements, e.g.\n    -   based on systematic drop-out [@pak_correcting_2022]\n    -   based on systematic misclassification of digital traces [@teblunthuis_etal2024]\n\nüëâ You know the drill: We will talk about this in session 4Ô∏è‚É£.\n\n## Step II: Data cleaning & augmentation\n\n![Figure. Data donation study - researcher perspective](images/ablauf-4b.jpg){fig-alt=\"process of data donation study\" fig-align=\"left\" width=\"300\"}\n\n## Step III: Modelling (Valerie)\n\n![Figure. Data donation study - researcher perspective](images/ablauf-4c.jpg){fig-alt=\"process of data donation study\" fig-align=\"left\" width=\"300\"}\n\n## Step III.I: How do I analyze results?\n\nThink carefully about...\n\n-   How to create indices from different metrics (e.g., liking, sharing, or commenting on content)\n-   Hierarchical structure (nested in time, metrics, platforms)\n-   Skewed data, non-linearity\n\n## Summary: Researcher perspective üìö\n\n-   **Summary**: Key steps include...\n\n    1.  Research design & tool set-up\n    2.  Data cleaning & augmentation\n    3.  Modelling\n\n-   **Further literature**:\n\n    -   @boeschoten_privacy-preserving_2022\n\n    -   @carriere_etal2024\n\n##\n\n::: {style=\"font-size: 400%;text-align:center;\"}\n**Questions?** ü§î\n:::\n\n## References\n","srcMarkdownNoYaml":"\n\n<h1>Follow the User?!</h1>\n\n<h3>Data Donation Studies for Collecting Digital Trace Data</h3>\n\n<hr>\n\nSession 3Ô∏è‚É£: Data Donation Studies (Researcher Perspective)\n\nFrieder Rodewald (University of Mannheim) & Valerie Hase (LMU Munich)\n\n<hr>\n\nüëâ Part of the SPP DFG Project [Integrating Data Donations in Survey Infrastructure](https://data-donation-science.de)\n\n##\n\n::: {style=\"font-size: 200%;text-align:center;\"}\n*What are methodological decisions researchers have to take in data donation studies?* ü§î\n:::\n\n## Data donation study - researcher perspective\n\n![Figure. Data donation study - researcher perspective](images/ablauf-4.jpg){fig-alt=\"process of data donation study\" fig-align=\"left\" width=\"300\"}\n\n## Agenda\n\n1. **Research design & tool set-up**\n\n2. **Data cleaning & augmentation**, including\n\n   üì¢ **Task 3**: Classify search terms\n\n3. **Modelling digital traces**\n\n![Image by Hope House Press via Unsplash](images/agenda.jpg){fig-align=\"left\" width=\"10%\"}\n\n## 1) Research design & tool set-up (Frieder)\n\n![Source: Image by Markus Winkler via Unsplash](images/methods.jpg){fig-alt=\"image of lupe\" fig-align=\"left\" width=\"300\"}\n\n## Step I: Research design & tool set-up\n\n![Figure. Data donation study - researcher perspective](images/ablauf-4a.jpg){fig-alt=\"process of data donation study\" fig-align=\"left\" width=\"300\"}\n\n## Step I: Research design & tool set-up\n\nKey decisions:\n\n-   Which theoretical questions do I want to answer?\n-   How do I operationalize key variables via my data donation tool?\n-   How do I integrate the tool in surveys & recruit participants?\n\n## Step I: Research design & tool set-up\n\nKey decisions:\n\n-   **Which theoretical questions do I want to answer?**\n-   How do I operationalize key variables via my data donation tool?\n-   How do I integrate the tool in surveys & recruit participants?\n\n## Step I.I Which questions do I want to answer?\n\nThis may sound silly but:\n\n-   Novel method, few empirical applications\n-   To date: methodological playground\n-   *What good is a method that is not used to advance theories/empirical knowledge?*\n\n::: {.notes}\n\n- If you have studied Twitter in the 2010's you know that this is pretty much what went wrong there, i.e., most often researchers had a nice data set and then looked for a suitable question to study.\n-> But the scientific way is better the other way around!\n-> Here it also makes a lot of sense to pre-register your study. You will also see in a moment why/how this helps you during programming.\n\n:::\n\n## Step I: Research design & tool set-up\n\nKey decisions:\n\n-   Which theoretical questions do I want to answer?\n-   **How do I operationalize key variables via my data donation tool?**\n-   How do I integrate the tool in surveys & recruit participants?\n\n## Step I.II: How do I operationalize key variables?\n\nChoose a tool, e.g., ...\n\n-   Port [@boeschoten_etal2023] (Netherlands, different platforms)\n-   Data Donation Module [@pfiffner_data_2022] (Switzerland, different platforms)\n-   WhatsR [@kohne_etal2024] (Germany, WhatsApp)\n\n::: {.notes}\n\n- Or you can program your own tool for your own application.\n- A good starting point is to look for data donation studies that were on the same platform; and use their scripts.\n\n:::\n\n## Step I.II: How do I operationalize key variables?\n\n-   Participants \"upload\" data\n-   Local extraction, anonymization, & aggregation\n-   Users can delete data\n-   Informed consent, only then: send to researcher server\n\n::: {.notes}\n\n- From a researcher perspective this makes it also appealing for ethics and data protection\n-> More cumbersome to explain but once they understand it, they like it.\n\n:::\n\n\n## Step I.II: How do I operationalize key variables?\n\n-   Participants \"upload\" data\n-   Local **extraction**, anonymization, & aggregation\n-   Users can delete data\n-   Informed consent, only then: send to researcher server\n\n## Step I.II: How do I operationalize key variables?\n\n<h3>Extractionüîé:</h3>\n\n![Figure. Filtering data - File extraction](images/extraction.jpg){fig-alt=\"Files in data donation packages\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\n<h3>Extractionüîé:</h3>\n\n![Figure. Filtering data - Python code](images/filtering.jpg){fig-alt=\"Python code for extracting files\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\n<h3>Extractionüîé:</h3>\n\n![Figure. Filtering data - Python code](images/filtering-a.jpg){fig-alt=\"Python code for extracting files\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\n-   Participants \"upload\" data\n-   Local extraction, **anonymization**, & aggregation\n-   Users can delete data\n-   Informed consent, only then: send to researcher server\n\n## Step I.II: How do I operationalize key variables?\n\n<h3>Anonymization üôà:</h3>\n\n![Figure. Anonymization - Example of Whitelists](images/anonymization.jpg){fig-alt=\"Example whitelists for news outlets\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\n<h3>Anonymization üôà:</h3>\n\n![Figure. Example of anonymized data](images/anonymization-a.jpg){fig-alt=\"Example of anonymized data\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\n-   Participants \"upload\" data\n-   Local extraction, anonymization, & **aggregation**\n-   Users can delete data\n-   Informed consent, only then: send to researcher server\n\n::: {.notes}\n\n- This often goes hand-in-hand with anonymization, but requires you to know your research question: If you aggregate too much here, you might not have all the information you need for your analyses.\n\n:::\n\n## Step I.II: How do I operationalize key variables?\n\n<h3>Aggregation üßÆ:</h3>\n\n![Figure. Aggregation - Python code](images/filtering-a.jpg){fig-alt=\"Python code for aggregation\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\n-   Participants \"upload\" data\n-   Local extraction, anonymization, & aggregation\n-   Users can **delete data**\n-   Informed consent, only then: send to researcher server\n\n## Step I.II: How do I operationalize key variables?\n\n<h3>Data deletion by users ‚ùå:</h3>\n\n![Figure. Data deletion](images/deletion.jpg){fig-alt=\"Example of how users can delete their data\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\nThis is how much \"fun\" testing DDTs is:\n\n![Figure. Github issues - Testing the tool](images/testing.jpg){fig-alt=\"Github screenshot of testing\" fig-align=\"left\" width=\"200\"}\n\n## Step I.II: How do I operationalize key variables?\n\nKey issues üö® [@hase_etal2024]\n\n-   Missing documentation by platforms (e.g., file structure)\n-   Sudden changes in DDPs\n-   Differences across languages & devices\n-   Insufficient in-tool classification (e.g., LLM integration)\n\n::: {.notes}\n\n- It might be nice to directly classify information during extraction (also in terms of data protection), but since all the data is extracted on the participants devices, this makes it really hard to implement because they would have to do the API calls from their devices, handle the computing power,...\n\n:::\n\n##\n::: {style=\"font-size: 200%;text-align:center;\"}\n**Let's have a look at the technical set-up üíª:**\n\n\\\n\\\n\n[https://github.com/eyra/mono](https://github.com/eyra/mono)\n[https://github.com/eyra/feldspar](https://github.com/eyra/feldspar)\n\n:::\n\n::: {.notes}\n\n- The example flow is here: \"[https://next.eyra.co/assignment/335/content](https://next.eyra.co/assignment/335/content)\"\n- This is just one example for a data donation tool, but arguably the most elaborate platform out there (that you can self-host)\n\n:::\n\n<!--\n---\n\n![Video. Next settings](images/next_settings.mov){fig-alt=\"Next settings video\" fig-align=\"left\" width=\"1400\"}\n\n---\n\n![Video. Next workflow](images/next_workflow.mov){fig-alt=\"Next workflow video\" fig-align=\"left\" width=\"1400\"}\n\n-->\n\n---\n\n![Figure. Next setup 1](images/next_1.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 2](images/next_2.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 3](images/next_3.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 4](images/next_4.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 5](images/next_5.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 6](images/next_6.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 7](images/next_7.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 8](images/next_8.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 9](images/next_9.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n---\n\n![Figure. Next setup 10](images/next_10.png){fig-alt=\"Next workflow screenshots\" fig-align=\"left\" width=\"200\"}\n\n::: {.notes}\n\n- ...and how does this actually work: Pyodide; we just install python in a participants browser. That how we can code python and work with a language that is more suited to data wrangling than JavaScript\n\n:::\n\n## Strategy to make the extraction work\n\n1. Take a look at the DDP; download it, best for multiple time periods and for different languages\n2. Understand the structure of the JSON or CSV.\n3. Get an example file running.\n4. Write the code for the extraction script.\n5. Test your script, first locally and then in the wild.\n6. Adapt your script.\n\n## Example: Extract list of subscriptions\n\n![Figure. subscriptions.csv](images/subscriptions_csv.jpeg.png){fig-alt=\"Screenshot of subscriptions.csv\" fig-align=\"left\" width=\"200\"}\n\n---\n\n<div style=\"font-size: 1.5em;\">\n\n```{.python}\n...\n    \"subscriptions\": {\n        \"extraction_function\": ef.extract_subscriptions,\n        \"possible_filenames\": [\"Abos.csv\", \"subscriptions.csv\"],\n        \"title\": {\n            \"en\": \"Which channels are you subscribed to?\",\n            \"de\": \"Welche Kan√§le haben Sie abonniert?\",\n            \"nl\": \"Op welke kanalen ben je geabonneerd?\",\n        },\n    },\n...\n\n```\n\n</div>\n\n---\n\n<div style=\"font-size: 1.5em;\">\n\n```{.python}\ndef extract_youtube_content_from_zip_folder(zip_file_path, possible_filenames):\n    \"\"\"Extract content from YouTube data export zip file using filenames\"\"\"\n\n    try:\n        with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n            # Get the list of file names in the zip file\n            filenames = zip_ref.namelist()\n            # Look for matching files\n            for possible_filename in possible_filenames:\n                for filename in filenames:\n                    if possible_filename in filename:\n                        try:\n                            # Process based on file extension\n                            if filename.endswith(\".json\"):\n                                with zip_ref.open(filename) as json_file:\n                                    json_content = json.loads(json_file.read())\n                                    return json_content\n                            elif file_name.endswith(\".csv\"):\n                                with zip_ref.open(file_name) as csv_file:\n                                    csv_content = pd.read_csv(csv_file)\n                                    return csv_content\n\n                        # Try the next matching file if there's an error\n                        except Exception as e:\n                            print(f\"Error reading file {file_name}: {e}\")\n                            continue\n            # If we've checked all files and found no match\n            print(f\"No file matching file '{possible_filenames}' found\")\n            return None\n    except Exception as e:\n        print(f\"Error extracting YouTube content: {e}\")\n        return None\n```\n</div>\n\n---\n\n<div style=\"font-size: 1.5em;\">\n\n```{.python}\ndef extract_subscriptions(subscriptions_csv): # csv file is read before\n    \"\"\"Extract YouTube channel subscriptions\"\"\"\n\n    # Define column name\n    if \"Kanaltitel\" in subscriptions_csv.columns:  # language sensitive\n        channel_column = \"Kanaltitel\"\n    else:\n        channel_column = \"Channel Title\"\n\n    # Define description\n    channel_name = \"Subscribed Channel\"\n\n    # Create DataFrame with just the channel names\n    subscriptions_df = pd.DataFrame({channel_name: subscriptions_csv[channel_column]})\n\n    return subscriptions_df\n```\n</div>\n---\n\n![Figure. Processed subscriptions.csv](images/subscriptions_donated.png){fig-alt=\"Screenshot of the processed subscriptions.csv\" fig-align=\"left\" width=\"200\"}\n\n## Step I: Research design & tool set-up\n\nKey decisions:\n\n-   Which theoretical questions do I want to answer?\n-   How do I operationalize key variables via my data donation tool?\n-   **How do I integrate the tool in surveys & recruit participants?**\n\n## Step I.III: How do I integrate the tool in surveys & recruit participants?\n\n-   Often: survey, then forwarding to an external site\n-   Less often: Integration in existing survey infrastructure [@haim_integrating_2023]\n\n::: {.notes}\n\n- The often use setup of \"first survey, then donation\" has the advantage that you know who participated in your study and then understand how your final sample of people who actually donate their data is biased.\n\n:::\n\n## Step I.III: How do I integrate the tool in surveys & recruit participants?\n\n::: incremental\n-   Low response rates [e.g., @hase_haim2024a; @keusch_etal2024]\n\n    - Behavioral intentions as \"willingness to donate\" high (79-52% of survey respondents)\n    - Actual behavior as \"participation in data donation\" low (37-12% of survey respondents)\n    - Well known intention-behavior gap [@kmetty_etal2025]\n\n-   Non-response bias\n\n-   Primary used in non-probability panels (e.g. online access panels)\n\n-   Survey design strategies: For now, ü§ë is the only thing that works.\n\n-   üëâ Again, we will talk about this in session 4Ô∏è‚É£.\n\n:::\n\n::: {.notes}\n\n- As you see this method, like every method, has its shortcomings.\n\n:::\n\n## Step I: Research design & tool set-up\n\n![Figure. Data donation study - researcher perspective](images/ablauf-4a.jpg){fig-alt=\"process of data donation study\" fig-align=\"left\" width=\"300\"}\n\n## Step II: Data cleaning & augmentation (Valerie)\n\n![Figure. Data donation study - researcher perspective](images/ablauf-4b.jpg){fig-alt=\"process of data donation study\" fig-align=\"left\" width=\"300\"}\n\n\n## Step II.I: How do I clean and extend data?\n\nThis is how your data may look like:\n\n![Figure. Donated data - example](images/cleaning_1.jpg){fig-alt=\"Example of donated data\" fig-align=\"left\" width=\"300\"}\n\n## Step II.I: How do I clean and extend data?\n\nThis is how your data may look like:\n\n![Figure. Donated data - example](images/cleaning_2.jpg){fig-alt=\"process of data donation study\" fig-align=\"left\" width=\"300\"}\n\n## Step II.I: How do I clean and extend data?\n\n-   Manual annotation by participants during data donation\n-   APIs/scraping to extend collected data\n-   Text-as-data methods for classification\n\n##\n::: {style=\"font-size: 100%;text-align:center;\"}\n**üì¢ Task 3: Classify search terms**\n\n*Download the data for Task 4 from the workshop website. This contains YouTube searches collected from a German social media sample. Either discuss this (no-code group) or do this in R/Python (code group).....*\n\n(1) How you would clean the data?\n\n(2) How you would identify health-related searches using NLP methods?\n\n:::\n\n![Figure. Donated data - example](images/sample-yt.jpg){fig-alt=\"Example of YouTube searches\" fig-align=\"left\" width=\"100\"}\n\n## Step II.II: How do I check for bias?\n\n-   Errors in representation and measurements, e.g.\n    -   based on systematic drop-out [@pak_correcting_2022]\n    -   based on systematic misclassification of digital traces [@teblunthuis_etal2024]\n\nüëâ You know the drill: We will talk about this in session 4Ô∏è‚É£.\n\n## Step II: Data cleaning & augmentation\n\n![Figure. Data donation study - researcher perspective](images/ablauf-4b.jpg){fig-alt=\"process of data donation study\" fig-align=\"left\" width=\"300\"}\n\n## Step III: Modelling (Valerie)\n\n![Figure. Data donation study - researcher perspective](images/ablauf-4c.jpg){fig-alt=\"process of data donation study\" fig-align=\"left\" width=\"300\"}\n\n## Step III.I: How do I analyze results?\n\nThink carefully about...\n\n-   How to create indices from different metrics (e.g., liking, sharing, or commenting on content)\n-   Hierarchical structure (nested in time, metrics, platforms)\n-   Skewed data, non-linearity\n\n## Summary: Researcher perspective üìö\n\n-   **Summary**: Key steps include...\n\n    1.  Research design & tool set-up\n    2.  Data cleaning & augmentation\n    3.  Modelling\n\n-   **Further literature**:\n\n    -   @boeschoten_privacy-preserving_2022\n\n    -   @carriere_etal2024\n\n##\n\n::: {style=\"font-size: 400%;text-align:center;\"}\n**Questions?** ü§î\n:::\n\n## References\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","self-contained":true,"number-sections":false,"highlight-style":"atom-one","output-file":"03-study-researcher-perspective.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.6.43","auto-stretch":true,"bibliography":["references/references.bib"],"csl":"references/apa.csl","footer":"Data Donation Studies - COMPTEXT - Frieder Rodewald, Valerie Hase","center-title-slide":false,"theme":["theme/q-theme.scss"],"code-copy":true,"code-block-height":"670px","smaller":true,"previewLinks":"auto","progress":true}}},"projectFormats":["html"]}